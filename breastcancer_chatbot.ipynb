{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomunizua/breast_cancer-qa_chatbot/blob/main/breastcancer_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha6FN-_x9AbZ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Install the Hugging Face datasets and other libraries\n",
        "!pip install -U datasets\n",
        "!pip install -U evaluate\n",
        "!pip install -U rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27cGBCAoiq6_"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "import evaluate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63CRAzFqp11C"
      },
      "source": [
        "## Loading of the dataset\n",
        "In this step, I loaded the grasool/breast-cancer-QAs-llama dataset from the Hugging Face Hub. To ensure smooth dataset caching, I defined a custom temporary cache directory. This ensures that the dataset is stored and accessed locally during multiple runs without needing to re-download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnEHN_3r5GSu"
      },
      "outputs": [],
      "source": [
        "# Define a temporary cache directory for Hugging Face datasets\n",
        "CACHE_DIR = \"/tmp/huggingface_datasets_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)  # Ensure the directory exists\n",
        "\n",
        "print(f\"Using cache directory: {CACHE_DIR}\")\n",
        "\n",
        "# Load the breast cancer QA dataset from Hugging Face\n",
        "dataset = load_dataset(\"grasool/breast-cancer-QAs-llama\", split=\"train\", cache_dir=CACHE_DIR)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Preview the first 5 examples to understand the structure\n",
        "preview_data = list(islice(dataset, 5))\n",
        "\n",
        "# Convert the preview data to a DataFrame for easier analysis\n",
        "df = pd.DataFrame(preview_data)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtyKsFFpq-_n"
      },
      "source": [
        "##Data cleaning and splitting\n",
        "I defined a clean_text() function that removes HTML tags and unnecessary whitespace using BeautifulSoup and regex, to ensure input text is consistent and free from web formatting issues, which could mislead the model.\n",
        "Upon inspection of the dataset, I found that it is structured in a prompt-response format using [INST]...[/INST]... so I created a function parse_llama_chat_format() to extract the QUESTION and ANSWER using regex.\n",
        "\n",
        "The dataset was converted to a Pandas DataFrame to easily identify and drop rows with missing or empty values in either the question or answer. The final cleaned dataset was then split into training and validation sets using a 90/10 ratio.\n",
        "\n",
        "I printed a sample QA pair from the training set to verify everything looked correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2JOIS7TVT_t"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning & Train/Validation Splits (Updated for FLAN-T5 and dataset format)\n",
        "\n",
        "# Define the cleaning function\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup.get_text(separator=' ')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Function to parse the specific dataset format\n",
        "def parse_llama_chat_format(text_entry):\n",
        "    match = re.search(r'\\[INST\\]\\s*(.*?)\\s*\\[/INST\\]\\s*(.*?)\\s*</s>', text_entry, re.DOTALL)\n",
        "    if match:\n",
        "        question = clean_text(match.group(1))\n",
        "        answer = clean_text(match.group(2))\n",
        "        return {'QUESTION': question, 'ANSWER': answer}\n",
        "    return None\n",
        "\n",
        "print(\"Parsing dataset entries into 'QUESTION' and 'ANSWER' columns...\")\n",
        "\n",
        "# Apply parsing and filter invalid entries\n",
        "parsed_data = [parse_llama_chat_format(entry['text']) for entry in dataset]\n",
        "parsed_data = [entry for entry in parsed_data if entry is not None]\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "qa_dataset = Dataset.from_list(parsed_data)\n",
        "print(f\"Parsed dataset with {len(qa_dataset)} QA pairs.\")\n",
        "\n",
        "# Handle missing or empty values\n",
        "temp_df = qa_dataset.to_pandas()\n",
        "original_rows = len(temp_df)\n",
        "\n",
        "temp_df.dropna(subset=['QUESTION', 'ANSWER'], inplace=True)\n",
        "temp_df = temp_df[temp_df['QUESTION'].str.strip() != '']\n",
        "temp_df = temp_df[temp_df['ANSWER'].str.strip() != '']\n",
        "\n",
        "dropped_rows = original_rows - len(temp_df)\n",
        "qa_dataset = Dataset.from_pandas(temp_df)\n",
        "\n",
        "if dropped_rows > 0:\n",
        "    print(f\"Dropped {dropped_rows} rows due to missing or empty 'QUESTION' or 'ANSWER' after parsing/cleaning.\")\n",
        "else:\n",
        "    print(\"No rows dropped due to missing or empty 'QUESTION' or 'ANSWER'.\")\n",
        "\n",
        "print(f\"\\nDataset after parsing and cleaning: {len(qa_dataset)} examples.\")\n",
        "\n",
        "# Train/Validation split\n",
        "train_test_split_result = qa_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset_hf = train_test_split_result['train']\n",
        "val_dataset_hf = train_test_split_result['test']\n",
        "\n",
        "print(f\"\\nTraining set size: {len(train_dataset_hf)}\")\n",
        "print(f\"Validation set size: {len(val_dataset_hf)}\")\n",
        "\n",
        "print(\"\\nExample of parsed and cleaned data from training set (first example):\")\n",
        "print(f\"QUESTION: {train_dataset_hf[0]['QUESTION']}\")\n",
        "print(f\"ANSWER: {train_dataset_hf[0]['ANSWER']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUdVAGXbt-bo"
      },
      "source": [
        "##Tokenization and Dataset Perparation\n",
        "In this step, I prepared the dataset for training the FLAN-T5 model by carefully tokenizing both the questions and answers. I used the T5Tokenizer from Hugging Face’s Transformers library, specifically the \"google/flan-t5-base\" checkpoint, because this model is instruction-tuned and well-suited for question-answering tasks without needing extra prefixes like \"question:\".\n",
        "\n",
        "I applied padding and truncation during tokenization to standardize sequence lengths, which is crucial for batching during training. For the target sequences (answers), I masked the padding tokens with a value of -100 so that the loss calculation ignores them — this is a standard technique to avoid penalizing the model for padded tokens.\n",
        "\n",
        "After tokenization, I converted the datasets into PyTorch tensors, which allows smooth integration with the Hugging Face Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrKGTZSrVlyD"
      },
      "outputs": [],
      "source": [
        "# Tokenization and Metric Setup\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Tokenization config (adjustable based on length analysis)\n",
        "MAX_INPUT_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 512\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['QUESTION']\n",
        "    targets = examples['ANSWER']\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # Mask padded label tokens with -100 so loss is not calculated on them\n",
        "    model_inputs[\"labels\"] = torch.where(\n",
        "        labels[\"attention_mask\"] == 0,\n",
        "        torch.tensor(-100, dtype=torch.long),\n",
        "        model_inputs[\"labels\"]\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to datasets\n",
        "tokenized_train_dataset = train_dataset_hf.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset_hf.map(preprocess_function, batched=True)\n",
        "\n",
        "# Format for PyTorch\n",
        "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_val_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Sample inspection\n",
        "print(\"\\nExample of tokenized input_ids (first question):\")\n",
        "print(tokenized_train_dataset[0]['input_ids'])\n",
        "\n",
        "print(\"\\nExample of tokenized attention_mask (first question):\")\n",
        "print(tokenized_train_dataset[0]['attention_mask'])\n",
        "\n",
        "print(\"\\nExample of tokenized labels (first answer):\")\n",
        "print(tokenized_train_dataset[0]['labels'])\n",
        "\n",
        "# Decoded versions\n",
        "print(\"\\nDecoded input_ids (first question):\")\n",
        "print(tokenizer.decode(tokenized_train_dataset[0]['input_ids'], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\nDecoded labels (first answer):\")\n",
        "print(tokenizer.decode(\n",
        "    [token for token in tokenized_train_dataset[0]['labels'].tolist() if token != -100],\n",
        "    skip_special_tokens=True\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9PxxqSCvf4O"
      },
      "source": [
        "##Defining evaluation metrics\n",
        "For evaluation, I chose three metrics to cover different aspects of answer quality:\n",
        "- BLEU, which captures n-gram overlap and fluency, using smoothing to handle edge cases,\n",
        "- SQuAD metrics (Exact Match and F1) that are standard for QA tasks measuring precise answer accuracy,\n",
        "- ROUGE scores (1, 2, and L), which assess the overlap of key phrases and the overall structure between predictions and references.\n",
        "\n",
        "Together, these metrics provide a comprehensive assessment of how well the model generates correct, fluent, and relevant answers in this specialized medical context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW3w2ulrtbFy"
      },
      "outputs": [],
      "source": [
        "# Evaluation metric setup\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
        "\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    predicted_token_ids = np.argmax(predictions, axis=-1)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # BLEU (robust with smoothing)\n",
        "    cleaned_preds = [p for p in decoded_preds if p.strip()]\n",
        "    cleaned_labels = [[l] for l in decoded_labels if l.strip()]\n",
        "\n",
        "    bleu_score = 0.0\n",
        "    if cleaned_preds and cleaned_labels:\n",
        "        bleu_results = bleu_metric.compute(\n",
        "            predictions=cleaned_preds,\n",
        "            references=cleaned_labels,\n",
        "            max_order=4,\n",
        "            smooth=True\n",
        "        )\n",
        "        bleu_score = bleu_results[\"bleu\"]\n",
        "\n",
        "    # SQuAD-style EM and F1\n",
        "    formatted_preds = [{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(decoded_preds)]\n",
        "    formatted_refs = [{\"id\": str(i), \"answers\": {\"answer_start\": [0], \"text\": [label]}} for i, label in enumerate(decoded_labels)]\n",
        "\n",
        "    squad_results = squad_metric.compute(predictions=formatted_preds, references=formatted_refs)\n",
        "\n",
        "    # ROUGE\n",
        "    rouge_results = rouge_metric.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[label] for label in decoded_labels]\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bleu\": round(bleu_score, 4),\n",
        "        \"squad_f1\": round(squad_results[\"f1\"], 4),\n",
        "        \"squad_exact_match\": round(squad_results[\"exact_match\"], 4),\n",
        "        \"rouge1_f1\": round(rouge_results[\"rouge1\"], 4),\n",
        "        \"rouge2_f1\": round(rouge_results[\"rouge2\"], 4),\n",
        "        \"rougeL_f1\": round(rouge_results[\"rougeL\"], 4)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNg-e-OE3RIk"
      },
      "source": [
        "To monitor training stability, I implemented a custom DebugCallback class that checks the model's gradients and parameters every 10 steps. This helped me detect issues like vanishing gradients, all-zero gradients (which indicate no learning), or NaN/Inf values that often signal numerical instability or divergence. By inspecting these values during training, I could proactively identify and fix issues like poor learning rates or batch sizes that were too small — making debugging more efficient and transparent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNtt9acFugXH"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "class DebugCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Custom callback to inspect gradients and model parameters during training.\n",
        "    Useful for debugging vanishing gradients, NaNs, or training divergence.\n",
        "    \"\"\"\n",
        "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if state.global_step % 10 == 0 and state.global_step > 0:\n",
        "            model = kwargs.get(\"model\")\n",
        "            if model is None:\n",
        "                print(\"DEBUG_CALLBACK: Model not found in kwargs.\")\n",
        "                return\n",
        "\n",
        "            print(f\"\\n--- DEBUG_CALLBACK (Step {state.global_step}) ---\")\n",
        "\n",
        "            grads_found = False\n",
        "            nan_grads = False\n",
        "            all_grads_zero = True\n",
        "            zero_threshold = 1e-9\n",
        "\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    grads_found = True\n",
        "                    if torch.isnan(param.grad).any():\n",
        "                        nan_grads = True\n",
        "                        print(f\"  WARNING: NaN gradient in {name}\")\n",
        "                    if param.grad.abs().sum().item() > zero_threshold:\n",
        "                        all_grads_zero = False\n",
        "\n",
        "            if not grads_found:\n",
        "                print(\"  CRITICAL: No gradients found! Check backward pass.\")\n",
        "            elif nan_grads:\n",
        "                print(\"  CRITICAL: NaNs detected in gradients.\")\n",
        "            elif all_grads_zero:\n",
        "                print(\"  CRITICAL: All gradients near zero — model may not be learning.\")\n",
        "            else:\n",
        "                print(\"  Good: Gradients appear non-zero.\")\n",
        "\n",
        "            nan_params = False\n",
        "            for name, param in model.named_parameters():\n",
        "                if torch.isnan(param).any():\n",
        "                    print(f\"  CRITICAL: NaN in parameter: {name}\")\n",
        "                    nan_params = True\n",
        "                if torch.isinf(param).any():\n",
        "                    print(f\"  CRITICAL: Inf in parameter: {name}\")\n",
        "                    nan_params = True\n",
        "\n",
        "            if nan_params:\n",
        "                print(\"  CRITICAL: Model parameters contain NaNs/Infs — training instability likely.\")\n",
        "\n",
        "            print(\"-----------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddZSJyw-QUTX"
      },
      "source": [
        "# Model training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvJltA20xvhP"
      },
      "source": [
        "## FLAN-T5-BASE Train 1\n",
        "In this section, I fine-tuned the FLAN-T5 base model on my cleaned question–answer dataset using the Hugging Face Trainer API. I specified appropriate training parameters including a low batch size to avoid memory issues, a learning rate of 3e-5, and set the evaluation and checkpoint saving strategy to run after each epoch. I also set the Trainer to load the best model at the end based on the lowest evaluation loss.\n",
        "\n",
        "After training, I evaluated the model on the validation set using the metrics set previously. I also calculated perplexity from the evaluation loss to assess how confidently the model generates predictions. The model and tokenizer were saved for future inference or deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k6RGhpuWULT"
      },
      "outputs": [],
      "source": [
        "# Trainer Setup and Training\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "LEARNING_RATE = 3e-5\n",
        "BATCH_SIZE = 2\n",
        "NUM_EPOCHS = 7\n",
        "OUTPUT_DIR = \"./flan_t5_breast_cancer_qa\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Load the FLAN-T5 model ---\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Define training arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "# --- Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Start training ---\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# --- Final Evaluation ---\n",
        "print(\"\\nStarting final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "for key, value in eval_results.items():\n",
        "    if key == \"eval_loss\":\n",
        "        print(f\"Loss: {value:.4f}\")\n",
        "    elif key == \"eval_bleu\":\n",
        "        print(f\"BLEU Score: {value:.4f}\")\n",
        "    elif key == \"eval_squad_f1\":\n",
        "        print(f\"F1 Score (SQuAD): {value:.4f}\")\n",
        "    elif key == \"eval_squad_exact_match\":\n",
        "        print(f\"Exact Match (SQuAD): {value:.4f}\")\n",
        "    elif key.startswith(\"eval_\"):\n",
        "        print(f\"{key.replace('eval_', '').replace('_', ' ').title()}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "# --- Perplexity Calculation ---\n",
        "if \"eval_loss\" in eval_results:\n",
        "    perplexity = np.exp(eval_results[\"eval_loss\"])\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n'eval_loss' not found in evaluation results. Cannot calculate perplexity.\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "# --- Save model and tokenizer ---\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\n✅ Model and tokenizer saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ryGUHwBQa5i"
      },
      "source": [
        "## FLAN-T5-BASE Train 2\n",
        "In this train, I fine-tuned the FLAN-T5 base model using a smaller batch size and a slightly higher learning rate of 1e-4 compared to the earlier 3e-5 to speed up convergence. I also extended the training duration to 10 epochs (previously 7), allowing the model more room to learn patterns from the data. One key change was increasing gradient_accumulation_steps to 8 to simulate a larger batch size and help stabilize training despite limited memory.\n",
        "\n",
        "This configuration is more aggressive, but it comes with a higher risk of overfitting, which is why I preserved evaluation and saving after every epoch. I also introduced a custom callback for debugging and used AdamW explicitly for optimization control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Qhejb6Sxf6V"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = 'google/flan-t5-base'\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "OUTPUT_DIR = \"./flan_t5_breast_cancer_qa_test2\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load model\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.0,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    gradient_accumulation_steps=8,\n",
        "    fp16=False,\n",
        ")\n",
        "\n",
        "# Custom optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[DebugCallback()],\n",
        "    optimizers=(optimizer, None),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nStarting final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "for key, value in eval_results.items():\n",
        "    if key == 'eval_loss':\n",
        "        print(f\"Loss: {value:.4f}\")\n",
        "    elif key == 'eval_bleu':\n",
        "        print(f\"BLEU Score: {value:.4f}\")\n",
        "    elif key == 'eval_squad_f1':\n",
        "        print(f\"F1 Score (SQuAD): {value:.4f}\")\n",
        "    elif key == 'eval_squad_exact_match':\n",
        "        print(f\"Exact Match (SQuAD): {value:.4f}\")\n",
        "    elif key.startswith('eval_'):\n",
        "        print(f\"{key.replace('eval_', '').replace('_', ' ').title()}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "# Perplexity calculation\n",
        "if 'eval_loss' in eval_results:\n",
        "    perplexity = np.exp(eval_results['eval_loss'])\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n'eval_loss' not found in evaluation results. Cannot calculate perplexity.\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"\\n✅ Model and tokenizer saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_c1Iv83bK6U"
      },
      "source": [
        "##FLAN-T5-BASE Train 3 (best performing model)\n",
        "For this final and best-performing model, I used google/flan-t5-base with a smaller batch size (1) and trained for 20 epochs to allow more learning cycles while monitoring performance after each epoch. I set the learning rate to 1e-4, which worked better than previous lower values by accelerating learning without destabilizing training. I also enabled gradient accumulation (8 steps) to simulate a larger effective batch size, as I was running out of memory.\n",
        "\n",
        "To ensure generalization and avoid overfitting, I added early stopping with a patience of 5 epochs. Compared to earlier attempts, this configuration balanced depth (more epochs) with caution (early stopping), and it showed clear improvements across metrics like BLEU, ROUGE, and SQuAD F1, as well as lower perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mq3tgOlbF-Y"
      },
      "outputs": [],
      "source": [
        "# Trainer Setup and Training\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = 'google/flan-t5-base'\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 20 # more epochs\n",
        "OUTPUT_DIR = \"./flan_t5_breast_cancer_qa_3\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load the model\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.05,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    gradient_accumulation_steps=8,\n",
        "    fp16=False,\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        "    optimizers=(optimizer, None),\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nStarting final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "for key, value in eval_results.items():\n",
        "    if key == 'eval_loss':\n",
        "        print(f\"Loss: {value:.4f}\")\n",
        "    elif key == 'eval_bleu':\n",
        "        print(f\"BLEU Score: {value:.4f}\")\n",
        "    elif key == 'eval_squad_f1':\n",
        "        print(f\"F1 Score (SQuAD): {value:.4f}\")\n",
        "    elif key == 'eval_squad_exact_match':\n",
        "        print(f\"Exact Match (SQuAD): {value:.4f}\")\n",
        "    elif key.startswith('eval_'):\n",
        "        print(f\"{key.replace('eval_', '').replace('_', ' ').title()}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "# Perplexity calculation\n",
        "if 'eval_loss' in eval_results:\n",
        "    perplexity = np.exp(eval_results['eval_loss'])\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n'eval_loss' not found in evaluation results. Cannot calculate perplexity.\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"\\n✅ Model and tokenizer saved to {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqVNWZ1MB5NG"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(\"./flan_t5_breast_cancer_qa_3\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_k0U3p1SOC4"
      },
      "source": [
        "## Qualitative Testing\n",
        "To evaluate the performance of my fine-tuned FLAN-T5 model, I loaded the best checkpoint and deployed it for qualitative testing. I implemented a question-answer generation function that mirrors the same input formatting and max token lengths used during training to maintain consistency. The model was configured to generate responses using beam search with a no_repeat_ngram_size to reduce repetition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZIxkR0HrdOA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# --- Configuration (match with training setup) ---\n",
        "OUTPUT_DIR = \"./flan_t5_breast_cancer_qa_3\"\n",
        "MAX_INPUT_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 512\n",
        "\n",
        "# --- Load Model and Tokenizer ---\n",
        "print(f\"Loading model and tokenizer from {OUTPUT_DIR}...\")\n",
        "try:\n",
        "    model = T5ForConditionalGeneration.from_pretrained(OUTPUT_DIR)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "    print(\"Model and tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or tokenizer: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Setup Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(f\"Model moved to: {device}\")\n",
        "\n",
        "# --- Answer Generation Function ---\n",
        "def generate_answer(question: str):\n",
        "    formatted_question = f\"question: {question}\"\n",
        "\n",
        "    input_ids = tokenizer.encode(\n",
        "        formatted_question,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2 # to prevent repetitive phrases\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# --- Qualitative Testing Example ---\n",
        "print(\"\\n--- Starting Qualitative Testing ---\")\n",
        "\n",
        "test_question = \"What are common treatments for early stage breast cancer?\"\n",
        "predicted_answer = generate_answer(test_question)\n",
        "print(f\"\\nQuestion: {test_question}\")\n",
        "print(f\"Answer: {predicted_answer}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- Interactive Testing ---\n",
        "print(\"\\n--- Interactive Testing (type 'exit' to quit) ---\")\n",
        "while True:\n",
        "    user_question = input(\"Enter your question: \")\n",
        "    if user_question.lower() == 'exit':\n",
        "        break\n",
        "    if not user_question.strip():\n",
        "        print(\"Please enter a question.\")\n",
        "        continue\n",
        "\n",
        "    answer = generate_answer(user_question)\n",
        "    print(f\"Bot: {answer}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"\\nQualitative testing complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvZpPcAIVQC8"
      },
      "source": [
        "## Initial training\n",
        "In the cells below, I started by training the model using the t5-small checkpoint. However, I quickly realized that both my preprocessing pipeline and the model itself were not well-suited for the complexity of the task. The T5-small model lacked the capacity to generate high-quality answers consistently, especially given the domain-specific nature of the breast cancer QA dataset. This resulted in low performance and high loss values. These were early-stage tests and do not reflect the final results—so feel free to skip them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjsnMc6i_WoB"
      },
      "outputs": [],
      "source": [
        "# Parse Q&A Pairs, Clean Text, and Create Train/Validation Splits\n",
        "import re\n",
        "\n",
        "MAIN_TEXT_COLUMN = dataset.column_names[0]\n",
        "\n",
        "# Define the parsing function\n",
        "def parse_qa_from_sft_format(example):\n",
        "    \"\"\"\n",
        "    Parses the 'text' column to extract 'question' and 'answer'.\n",
        "    Handles the <s>[INST] Q [/INST] A </s> format.\n",
        "    \"\"\"\n",
        "    text = example[MAIN_TEXT_COLUMN]\n",
        "\n",
        "    # Regex to find the pattern: <s>[INST] QUESTION_TEXT [/INST] ANSWER_TEXT </s>\n",
        "    # re.DOTALL allows '.' to match newlines, important if Q/A spans multiple lines\n",
        "    match = re.fullmatch(r'<s>\\[INST\\] (.*?) \\[/INST\\] (.*?) </s>', text, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        question = match.group(1).strip()\n",
        "        answer = match.group(2).strip()\n",
        "        return {\"question\": question, \"answer\": answer}\n",
        "    else:\n",
        "        # If a line doesn't match the format, return None for filtering\n",
        "        print(f\"Warning: Could not parse example: {text[:100]}...\") # Print problematic lines\n",
        "        return {\"question\": None, \"answer\": None}\n",
        "\n",
        "# Apply parsing to create 'question' and 'answer' columns\n",
        "print(\"\\nParsing 'text' column into 'question' and 'answer' columns...\")\n",
        "parsed_dataset = dataset.map(parse_qa_from_sft_format, remove_columns=[MAIN_TEXT_COLUMN]) # Remove original columns\n",
        "print(\"Parsing complete. Filtering out unparsed examples...\")\n",
        "\n",
        "# Filter out any examples that failed parsing\n",
        "parsed_dataset = parsed_dataset.filter(lambda example: example['question'] is not None and example['answer'] is not None)\n",
        "print(f\"Dataset after parsing and filtering: {len(parsed_dataset)} examples.\")\n",
        "\n",
        "# Define the cleaning function (re-used from previous iterations)\n",
        "def clean_text_for_qa(text):\n",
        "    text = str(text) # Ensure it's a string\n",
        "    text = text.lower() # Lowercasing\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove excessive whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the new 'question' and 'answer' columns\n",
        "print(\"Applying cleaning function to 'question' and 'answer' columns...\")\n",
        "cleaned_dataset = parsed_dataset.map(lambda example: {\n",
        "    'question': clean_text_for_qa(example['question']),\n",
        "    'answer': clean_text_for_qa(example['answer'])\n",
        "})\n",
        "print(\"Cleaning complete.\")\n",
        "\n",
        "# Now, perform the train/validation split on the cleaned_dataset\n",
        "train_test_split = cleaned_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "train_dataset_hf = train_test_split['train']\n",
        "val_dataset_hf = train_test_split['test']\n",
        "\n",
        "print(f\"\\nTraining set size: {len(train_dataset_hf)}\")\n",
        "print(f\"Validation set size: {len(val_dataset_hf)}\")\n",
        "\n",
        "print(\"\\nExample of parsed and cleaned data from training set (first example):\")\n",
        "print(f\"Question: {train_dataset_hf[0]['question']}\")\n",
        "print(f\"Answer: {train_dataset_hf[0]['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7CGPLb0D-My"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Add the task prefix to the input questions\n",
        "    inputs = [f\"question: {q}\" for q in examples['question']]\n",
        "    targets = examples['answer']\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length= 128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (answers)\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length= 380,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # T5 models expect the labels to be named 'labels'\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # For T5, we also need to replace padding token id in labels with -100\n",
        "    # This is because -100 is ignored by the loss function.\n",
        "    model_inputs[\"labels\"] = torch.where(\n",
        "        labels[\"attention_mask\"] == 0,\n",
        "        torch.tensor(-100, dtype=torch.long),\n",
        "        model_inputs[\"labels\"]\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function to the Hugging Face Datasets iusing .map()\n",
        "tokenized_train_dataset = train_dataset_hf.map(preprocess_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset_hf.map(preprocess_function, batched=True)\n",
        "\n",
        "# Set the format of the datasets to PyTorch\n",
        "# This ensures that when the Trainer accesses examples, they are already PyTorch tensors\n",
        "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_val_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "\n",
        "print(\"\\nExample of tokenized input_ids (first question):\")\n",
        "print(tokenized_train_dataset[0]['input_ids'])\n",
        "print(\"\\nExample of tokenized attention_mask (first question):\")\n",
        "print(tokenized_train_dataset[0]['attention_mask'])\n",
        "print(\"\\nExample of tokenized labels (first answer):\")\n",
        "print(tokenized_train_dataset[0]['labels'])\n",
        "\n",
        "print(\"\\nDecoded input_ids (first question):\")\n",
        "print(tokenizer.decode(tokenized_train_dataset[0]['input_ids'], skip_special_tokens=True))\n",
        "print(\"\\nDecoded labels (first answer):\")\n",
        "# Note: -100 values will be skipped in decoding\n",
        "print(tokenizer.decode([token for token in tokenized_train_dataset[0]['labels'].tolist() if token != -100], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "075E1h71FkpN"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# --- Define compute_metrics function for the Trainer ---\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
        "\n",
        "    if isinstance(predictions, tuple):\n",
        "        predictions = predictions[0]\n",
        "\n",
        "    predicted_token_ids = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    bleu_results = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
        "    bleu_score = bleu_results[\"bleu\"]\n",
        "\n",
        "    formatted_predictions = [{\"id\": str(i), \"prediction_text\": pred} for i, pred in enumerate(decoded_preds)]\n",
        "    formatted_references = [{\"id\": str(i), \"answers\": {\"answer_start\": [0], \"text\": [label]}} for i, label in enumerate(decoded_labels)]\n",
        "\n",
        "    squad_results = squad_metric.compute(predictions=formatted_predictions, references=formatted_references)\n",
        "    squad_f1 = squad_results[\"f1\"]\n",
        "    squad_em = squad_results[\"exact_match\"]\n",
        "\n",
        "    return {\n",
        "        \"bleu\": round(bleu_score, 4),\n",
        "        \"squad_f1\": round(squad_f1, 4),\n",
        "        \"squad_exact_match\": round(squad_em, 4)\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIgqhJPYEH4m"
      },
      "outputs": [],
      "source": [
        "# Trainer Setup and Training\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = 't5-small'\n",
        "LEARNING_RATE = 2e-5\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 3\n",
        "# # Output directory for saving model checkpoints\n",
        "OUTPUT_DIR = \"./t5_breast_cancer_chatbot\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# --- Load model ---\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1, # Saves only the last checkpoint\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"eval_loss\", # Metric to use for load_best_model_at_end\n",
        ")\n",
        "\n",
        "# --- Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Train ---\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nStarting evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# --- Save model and tokenizer ---\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"\\n✅ Model and tokenizer saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVmtRrJ3Fvnc"
      },
      "outputs": [],
      "source": [
        "# ]Model Evaluation and Perplexity Calculation\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nStarting final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "# Print each metric with a clear label and formatted value\n",
        "for key, value in eval_results.items():\n",
        "    if key == 'eval_loss':\n",
        "        print(f\"Loss: {value:.4f}\")\n",
        "    elif key == 'eval_bleu': # Key name depends on what your compute_metrics returned\n",
        "        print(f\"BLEU Score: {value:.4f}\")\n",
        "    elif key == 'eval_squad_f1':\n",
        "        print(f\"F1 Score (SQuAD): {value:.4f}\")\n",
        "    elif key == 'eval_squad_exact_match':\n",
        "        print(f\"Exact Match (SQuAD): {value:.4f}\")\n",
        "    elif key.startswith('eval_'): # For any other 'eval_' metrics\n",
        "        print(f\"{key.replace('eval_', '').replace('_', ' ').title()}: {value:.4f}\")\n",
        "    else: # For runtime, samples_per_second etc.\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "\n",
        "# --- Perplexity Calculation (from eval_loss) ---\n",
        "if 'eval_loss' in eval_results:\n",
        "    perplexity = np.exp(eval_results['eval_loss'])\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n'eval_loss' not found in evaluation results. Cannot calculate perplexity.\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrajWNjmvzNQ"
      },
      "outputs": [],
      "source": [
        "# Trainer Setup and Training\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "# --- Configuration (for tuning) ---\n",
        "MODEL_NAME = 't5-small'\n",
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 5\n",
        "# Output directory for saving model checkpoints\n",
        "OUTPUT_DIR = \"./t5_breast_cancer_chatbot_tuned_v1\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# --- Load model ---\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,  # 10% of total steps)\n",
        "    save_total_limit=1, # Saves only the last checkpoint\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False, # For eval_loss, smaller is better\n",
        ")\n",
        "\n",
        "# --- Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Train ---\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nStarting evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# --- Save model and tokenizer ---\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"\\n✅ Model and tokenizer saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bBHeAYxCJLf"
      },
      "outputs": [],
      "source": [
        "# ]Model Evaluation and Perplexity Calculation\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nStarting final evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\n--- Evaluation Results ---\")\n",
        "# Print each metric with a clear label and formatted value\n",
        "for key, value in eval_results.items():\n",
        "    if key == 'eval_loss':\n",
        "        print(f\"Loss: {value:.4f}\")\n",
        "    elif key == 'eval_bleu': # Key name depends on what your compute_metrics returned\n",
        "        print(f\"BLEU Score: {value:.4f}\")\n",
        "    elif key == 'eval_squad_f1':\n",
        "        print(f\"F1 Score (SQuAD): {value:.4f}\")\n",
        "    elif key == 'eval_squad_exact_match':\n",
        "        print(f\"Exact Match (SQuAD): {value:.4f}\")\n",
        "    elif key.startswith('eval_'): # For any other 'eval_' metrics\n",
        "        print(f\"{key.replace('eval_', '').replace('_', ' ').title()}: {value:.4f}\")\n",
        "    else: # For runtime, samples_per_second etc.\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "\n",
        "# --- Perplexity Calculation (from eval_loss) ---\n",
        "if 'eval_loss' in eval_results:\n",
        "    perplexity = np.exp(eval_results['eval_loss'])\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "else:\n",
        "    print(\"\\n'eval_loss' not found in evaluation results. Cannot calculate perplexity.\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_RPsYWpDqCA"
      },
      "outputs": [],
      "source": [
        "# --- Configuration (for tuning) ---\n",
        "MODEL_NAME = 't5-small' # Keep t5-small for now\n",
        "LEARNING_RATE = 3e-5     # Increase slightly from 1e-5\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10           # Increased epochs to allow more time to converge\n",
        "# # Output directory for saving model checkpoints\n",
        "OUTPUT_DIR = \"./t5_chatbot_tuned_v2\"\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "# --- Load model ---\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# --- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True, # Keep this enabled!\n",
        "    metric_for_best_model=\"eval_loss\", # Ensure this is 'eval_loss' for smaller is better\n",
        "    greater_is_better=False, # This is crucial for eval_loss\n",
        ")\n",
        "\n",
        "# --- Trainer --- (No changes here, use your existing setup)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# --- Train ---\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nStarting evaluation...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# --- Save model and tokenizer ---\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"\\n✅ Model and tokenizer saved to {OUTPUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ddZSJyw-QUTX",
        "fvZpPcAIVQC8"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}